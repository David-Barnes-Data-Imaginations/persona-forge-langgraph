"""
Simplified Gradio chat interface for LangGraph ReactAgent integration with FastAPI.
This interface is designed to work with your existing chat_agent.py setup.
Enhanced with voice functionality (STT/TTS).
"""

import gradio as gr
from typing import Generator, List, Tuple
import asyncio
import base64
import io
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver
from ..graphs.chat_agent import get_new_agent
from ..io_py.edge.config import LLMConfigVoice
from ..voice_service_faster import faster_whisper_service


class OutputChunkBuilder:
    """Builds output chunks for streaming TTS - speaks sentence by sentence"""

    def __init__(self):
        self._msg = ""
        self.end_of_sentence = (".", "?", ";", "!", "\n")

    def add_chunk(self, message_chunk: str):
        """Add a message chunk to the buffer"""
        self._msg += message_chunk

    def output_chunk_ready(self) -> bool:
        """Check if we have a complete sentence ready to speak"""
        return self._msg.endswith(self.end_of_sentence)

    def current_message_length(self) -> int:
        """Get length of current message buffer"""
        return len(self._msg.strip())

    def get_output_chunk(self):
        """Get the current chunk and reset the buffer"""
        msg = self._msg.strip()
        self._reset_message()
        return msg

    def _reset_message(self):
        """Reset the message buffer"""
        self._msg = ""


class LangGraphChatInterface:
    """Simplified chat interface for LangGraph ReactAgent"""

    def __init__(self):
        self.agent = None
        self.memory = MemorySaver()
        self.config = {"configurable": {"thread_id": "chat_session_1"}}
        self._initialize_agent()

    def _initialize_agent(self):
        """Initialize the LangGraph agent"""
        try:
            # Initialize with empty memory stores for now
            short_term_memory = MemorySaver()
            long_term_memory = None  # You can add a proper store later

            self.agent = get_new_agent(
                config=LLMConfigVoice,
                short_term_memory=short_term_memory,
                long_term_memory=long_term_memory,
            )
            print("‚úÖ LangGraph agent initialized successfully")
        except Exception as e:
            print(f"‚ùå Error initializing agent: {e}")
            self.agent = None

    def chat_response(
        self, message: str, history: List[List[str]]
    ) -> Generator[str, None, None]:
        """
        Generate streaming response from the LangGraph agent

        Args:
            message: User's input message
            history: Chat history as list of [user, assistant] pairs

        Yields:
            Streaming response chunks
        """
        if not self.agent:
            yield "‚ùå Agent not initialized. Please check the configuration."
            return

        try:
            # Convert history to LangChain messages if needed
            messages = []
            for user_msg, assistant_msg in history:
                messages.append(HumanMessage(content=user_msg))
                if assistant_msg:
                    messages.append(AIMessage(content=assistant_msg))

            # Add current message:
            messages.append(HumanMessage(content=message))

            # Stream response from agent
            full_response = ""
            for chunk in self.agent.stream(
                {"messages": messages}, config=self.config, stream_mode="values"
            ):
                if "messages" in chunk:
                    latest_message = chunk["messages"][-1]
                    if hasattr(latest_message, "content") and latest_message.content:
                        content = latest_message.content
                        if (
                            content != full_response
                        ):  # Only yield if content has changed
                            full_response = content
                            yield content

            # If no streaming occurred, get the final response
            if not full_response:
                result = self.agent.invoke({"messages": messages}, config=self.config)
                if "messages" in result and result["messages"]:
                    final_content = result["messages"][-1].content
                    yield final_content

        except Exception as e:
            error_msg = f"‚ùå Error getting response: {str(e)}"
            print(error_msg)
            yield error_msg

    def create_interface(self) -> gr.Blocks:
        """Create the Gradio chat interface"""

        with gr.Blocks(
            title="LangGraph Chat Agent",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 800px !important;
                margin: auto !important;
            }
            .chat-bubble {
                max-width: 70% !important;
            }
            """,
        ) as interface:

            # Voice status check
            voice_available = faster_whisper_service.is_available()
            voice_status_emoji = "üîä" if voice_available else "üîá"

            gr.Markdown(
                f"""
                # ü§ñ LangGraph Chat Agent {voice_status_emoji}

                Chat with your LangGraph ReactAgent powered by **{LLMConfigVoice.model_name}**

                This interface connects to your existing psychological analysis workflows.

                **Voice Features Available:** {'‚úÖ STT & TTS enabled' if voice_available else '‚ùå Voice disabled'}
                """
            )

            # Chat interface
            chatbot = gr.Chatbot(
                label="Chat with Agent",
                height=500,
                bubble_full_width=False,
                show_copy_button=True,
                layout="panel",
                type="messages",
            )

            # Input controls with voice
            with gr.Row():
                msg_input = gr.Textbox(
                    placeholder="Type your message here...",
                    label="Message",
                    scale=3,
                    lines=1,
                )

                # Voice input (only show if available)
                if voice_available:
                    with gr.Column(scale=1):
                        # Use numpy type for reliable audio capture
                        voice_input = gr.Audio(
                            sources=["microphone"],
                            type="numpy",
                            label="üé§ Voice Input",
                            show_download_button=False,
                            interactive=True,
                        )
                        # Manual transcribe button for reliable control
                        transcribe_btn = gr.Button(
                            "üìù Transcribe", variant="primary", size="sm"
                        )

                        # VAD toggle button
                        vad_toggle = gr.Button(
                            "üéôÔ∏è Voice Mode", variant="secondary", size="sm"
                        )
                else:
                    voice_input = None
                    transcribe_btn = None
                    vad_toggle = None

                send_btn = gr.Button("Send", scale=1, variant="primary")

            # Control buttons
            with gr.Row():
                clear_btn = gr.Button("Clear Chat", variant="secondary")
                reset_btn = gr.Button("Reset Agent Memory", variant="stop")

                # Voice controls (only show if available)
                if voice_available:
                    tts_enabled = gr.Checkbox(
                        label="üîä Enable Text-to-Speech", value=False, scale=1
                    )
                    # Test button to verify voice service
                    voice_test_btn = gr.Button(
                        "üîß Test Voice", variant="secondary", size="sm"
                    )
                else:
                    tts_enabled = None
                    voice_test_btn = None

            # Audio output for TTS
            if voice_available:
                audio_output = gr.Audio(
                    label="üîä Assistant Voice",
                    autoplay=True,
                    show_download_button=False,
                    interactive=False,
                )
            else:
                audio_output = None

            # Status indicators
            with gr.Row():
                status = gr.Textbox(
                    value=(
                        "üü¢ Agent Ready" if self.agent else "üî¥ Agent Not Initialized"
                    ),
                    label="Status",
                    interactive=False,
                    scale=2,
                )

                if voice_available:
                    vad_status = gr.Textbox(
                        value="üî¥ Voice Mode Off",
                        label="Voice Mode",
                        interactive=False,
                        scale=1,
                    )
                else:
                    vad_status = None

            # Event handlers
            def handle_voice_input(voice_file, current_text):
                """Handle voice input and update text field - simplified approach"""
                try:
                    print(f"üî• VOICE EVENT TRIGGERED!")
                    print(f"   - File: {voice_file}")

                    # Quick validation
                    if not voice_file:
                        print("‚ö†Ô∏è No voice file provided")
                        return current_text if current_text else ""

                    # Convert current_text to string safely
                    if current_text is None:
                        current_text = ""

                    # For now, just return a test message to verify the event works
                    # This bypasses the heavy Whisper processing that's causing timeouts
                    test_message = (
                        " [Voice recorded - processing temporarily disabled for demo]"
                    )

                    print(f"üìù Returning test message")
                    return current_text + test_message

                    # TODO: Re-enable actual transcription once event handling is stable
                    # The commented code below would do the actual transcription:

                    # import os
                    # if not os.path.exists(voice_file):
                    #     return current_text + " [File not found]"

                    # from ..voice_service import voice_service as vs
                    # if hasattr(vs, 'whisper_model') and vs.whisper_model:
                    #     result = vs.whisper_model.transcribe(voice_file, language="en")
                    #     transcribed_text = result["text"].strip()
                    #     return current_text + " " + transcribed_text if current_text else transcribed_text

                except Exception as e:
                    print(f"‚ùå Voice handler error: {e}")
                    return (
                        current_text if current_text else ""
                    ) + f" [Error: {str(e)[:30]}]"

            def test_voice_service():
                """Test function to verify voice service is working"""
                print("üß™ Testing voice service...")
                try:
                    if faster_whisper_service.is_available():
                        print("‚úÖ Voice service model found")
                        return f"‚úÖ Voice service is working! Model loaded successfully on {faster_whisper_service.device}."
                    else:
                        print("‚ùå Voice service model not found")
                        return "‚ùå Voice service model not available"
                except Exception as e:
                    print(f"‚ùå Voice service test error: {e}")
                    return f"‚ùå Voice service error: {str(e)}"

            def speak_text_chunk(text_chunk: str):
                """Generate TTS and play a single text chunk"""
                if not text_chunk.strip():
                    return

                try:
                    import requests
                    import tempfile
                    import subprocess

                    print(f"üîä Speaking chunk: '{text_chunk[:50]}...'")

                    # Call TTS API
                    response = requests.post(
                        "http://localhost:8000/api/voice/synthesize",
                        params={"text": text_chunk.strip()},
                        timeout=60,
                    )

                    if response.status_code == 200:
                        # Save audio to temporary file
                        with tempfile.NamedTemporaryFile(
                            suffix=".wav", delete=False
                        ) as tmp_file:
                            tmp_file.write(response.content)

                            # Play the audio file using aplay
                            try:
                                subprocess.run(
                                    ["aplay", tmp_file.name],
                                    capture_output=True,
                                    timeout=60,
                                )
                                print("‚úÖ Chunk playback completed")
                            except Exception as e:
                                print(f"‚ùå Chunk playback error: {e}")

                            # Clean up temp file
                            try:
                                import os

                                os.unlink(tmp_file.name)
                            except:
                                pass

                    else:
                        print(f"‚ùå TTS chunk error: {response.status_code}")

                except Exception as e:
                    print(f"‚ùå TTS chunk generation failed: {e}")

            def test_tts():
                """Test TTS functionality"""
                print("üîä Testing TTS...")
                try:
                    import requests
                    import tempfile

                    test_text = "Hello! This is a test of the text to speech system."
                    response = requests.post(
                        "http://localhost:8000/api/voice/synthesize",
                        params={"text": test_text},
                        timeout=10,
                    )

                    if response.status_code == 200:
                        # Save audio to temporary file
                        with tempfile.NamedTemporaryFile(
                            suffix=".wav", delete=False
                        ) as tmp_file:
                            tmp_file.write(response.content)
                            print(f"‚úÖ TTS test successful: {tmp_file.name}")

                            # Play the test audio
                            try:
                                import subprocess

                                print(f"üîä Playing test TTS audio...")
                                subprocess.run(
                                    ["aplay", tmp_file.name],
                                    capture_output=True,
                                    timeout=10,
                                )
                                print("‚úÖ Test audio playback completed")
                            except Exception as e:
                                print(f"‚ùå Test audio playback error: {e}")

                            return tmp_file.name
                    else:
                        print(f"‚ùå TTS test failed: {response.status_code}")
                        return None

                except Exception as e:
                    print(f"‚ùå TTS test error: {e}")
                    return None

            def transcribe_current_audio(voice_data, current_text):
                """Manually transcribe the current audio data (numpy format)"""
                print("üéôÔ∏è MANUAL TRANSCRIBE BUTTON PRESSED!")
                print(f"   Voice data type: {type(voice_data)}")
                print(f"   Current text: {current_text}")

                if voice_data is None:
                    print("‚ö†Ô∏è No audio data to transcribe")
                    return (current_text or "") + " [No audio data]"

                try:
                    # Use the faster-whisper service
                    if not faster_whisper_service.is_available():
                        print("‚ùå Voice service not available")
                        return (current_text or "") + " [Voice service not ready]"

                    # Use the numpy-based transcription
                    transcribed_text = faster_whisper_service.transcribe_audio_numpy(
                        voice_data
                    )

                    if "error" in transcribed_text.lower():
                        return (
                            current_text or ""
                        ) + f" [Error: {transcribed_text[:50]}]"

                    print(f"‚úÖ Manual transcription successful: '{transcribed_text}'")

                    # Combine with existing text
                    if current_text and current_text.strip():
                        final_text = f"{current_text} {transcribed_text}"
                    else:
                        final_text = transcribed_text

                    return final_text

                except Exception as e:
                    print(f"‚ùå Manual transcription error: {e}")
                    import traceback

                    traceback.print_exc()
                    return (current_text or "") + f" [Error: {str(e)[:30]}]"

            def respond(message, history, tts_enabled_value):
                """Handle user message and generate response with streaming TTS"""
                if not message.strip():
                    return history, "", None

                # Add user message to history
                history = history + [{"role": "user", "content": message}]

                # Initialize TTS streaming if enabled
                output_chunk_builder = (
                    OutputChunkBuilder() if tts_enabled_value else None
                )
                previously_spoken_length = 0

                # Get response from agent
                # full_response = ""
                # Convert history to old format for chat_response
                history_tuples = []
                for i in range(0, len(history) - 1, 2):
                    user_msg = history[i]["content"] if i < len(history) else ""
                    assistant_msg = (
                        history[i + 1]["content"] if i + 1 < len(history) else ""
                    )
                    history_tuples.append([user_msg, assistant_msg])

                # Add empty assistant response first
                history = history + [{"role": "assistant", "content": ""}]

                # Stream and accumulate response chunks with sentence-by-sentence TTS
                for chunk in self.chat_response(message, history_tuples):
                    if chunk and chunk.strip():
                        # For streaming TTS, we want to track only the new content
                        previous_response = (
                            history[-1]["content"] if history[-1]["content"] else ""
                        )

                        # Accumulate the full response
                        full_response = (
                            chunk  # chat_response already returns the full content
                        )
                        # Update the assistant's response in history
                        history[-1]["content"] = full_response

                        # Handle streaming TTS if enabled - only process new text
                        if tts_enabled_value and output_chunk_builder:
                            # Calculate what's new since last iteration
                            if len(full_response) > len(previous_response):
                                new_text = full_response[len(previous_response) :]

                                print(f"üîç New text chunk: '{new_text[:50]}...'")

                                # Smart filtering: skip only exact user input echoes
                                user_input_words = set(message.strip().lower().split())
                                new_text_words = set(new_text.strip().lower().split())

                                # Calculate overlap percentage
                                if user_input_words and new_text_words:
                                    overlap = len(
                                        user_input_words.intersection(new_text_words)
                                    )
                                    overlap_percentage = overlap / len(user_input_words)
                                else:
                                    overlap_percentage = 0

                                # Skip only if it's clearly just echoing the user input (high overlap)
                                # AND it's not a typical LLM response pattern
                                is_user_echo = (
                                    overlap_percentage > 0.1
                                    and len(new_text.strip()) < len(message) + 50
                                    and not any(
                                        word in new_text.lower()
                                        for word in [
                                            "yes",
                                            "hello",
                                            "sure",
                                            "i can",
                                            "i see",
                                            "here",
                                            "the",
                                            "my",
                                            "great",
                                        ]
                                    )
                                )

                                if not is_user_echo:
                                    print(
                                        f"‚úÖ Processing LLM chunk: '{new_text[:50]}...'"
                                    )

                                    # Add only the genuinely new LLM text to our chunk builder
                                    output_chunk_builder.add_chunk(new_text)

                                    # If we have a complete sentence, speak it
                                    if output_chunk_builder.output_chunk_ready():
                                        sentence = (
                                            output_chunk_builder.get_output_chunk()
                                        )
                                        if sentence and sentence.strip():
                                            print(
                                                f"üé§ Speaking LLM sentence: '{sentence[:50]}...'"
                                            )

                                            # Speak this sentence in a background thread to avoid blocking
                                            import threading

                                            thread = threading.Thread(
                                                target=speak_text_chunk,
                                                args=(sentence,),
                                                daemon=True,
                                            )
                                            thread.start()
                                else:
                                    print(
                                        f"üö´ Skipping user input echo (overlap: {overlap_percentage:.1%}): '{new_text[:50]}...'"
                                    )

                        yield history, "", None

                # Handle any remaining text in the buffer
                if (
                    tts_enabled_value
                    and output_chunk_builder
                    and output_chunk_builder.current_message_length() >= 0
                ):
                    remaining_text = output_chunk_builder.get_output_chunk()
                    if remaining_text and remaining_text.strip():
                        print(f"üé§ Speaking remaining text: '{remaining_text[:50]}...'")
                        import threading

                        thread = threading.Thread(
                            target=speak_text_chunk, args=(remaining_text,), daemon=True
                        )
                        thread.start()

                # Clear the message input and return final state
                return history, "", None

            def clear_chat():
                """Clear the chat history"""
                return [], "üü¢ Chat cleared"

            def reset_memory():
                """Reset the agent's memory"""
                try:
                    # Reinitialize the agent to clear memory
                    self._initialize_agent()
                    return [], "üü¢ Agent memory reset"
                except Exception as e:
                    return [], f"‚ùå Error resetting memory: {str(e)}"

            # Create a wrapper function for when TTS is not available
            def respond_no_tts(message, history):
                return respond(message, history, False)

            # Wire up events - with TTS support
            if voice_available:
                send_btn.click(
                    respond,
                    inputs=[msg_input, chatbot, tts_enabled],
                    outputs=[chatbot, msg_input, audio_output],
                )

                msg_input.submit(
                    respond,
                    inputs=[msg_input, chatbot, tts_enabled],
                    outputs=[chatbot, msg_input, audio_output],
                )
            else:
                send_btn.click(
                    respond_no_tts,
                    inputs=[msg_input, chatbot],
                    outputs=[chatbot, msg_input],
                )

                msg_input.submit(
                    respond_no_tts,
                    inputs=[msg_input, chatbot],
                    outputs=[chatbot, msg_input],
                )

            # Voice transcription - Manual button approach for reliability
            if voice_available and transcribe_btn:
                transcribe_btn.click(
                    transcribe_current_audio,
                    inputs=[voice_input, msg_input],
                    outputs=[msg_input],
                )

            clear_btn.click(clear_chat, outputs=[chatbot, status])

            reset_btn.click(reset_memory, outputs=[chatbot, status])

            # Voice test button
            if voice_available and voice_test_btn:
                voice_test_btn.click(test_tts, outputs=[audio_output])

            # VAD toggle functionality
            def toggle_vad():
                """Toggle VAD mode on/off"""
                # This will be handled by JavaScript, just return a status update
                return "üü° Toggling Voice Mode..."

            if voice_available and vad_toggle:
                vad_toggle.click(toggle_vad, outputs=[vad_status])

            # Add custom JavaScript for VAD functionality
            interface.load(
                None,
                None,
                None,
                js="""
                function() {
                    // VAD JavaScript implementation
                    let audioCtx, mic, workletNode, socket;
                    let speaking = false;
                    let silenceMs = 0;
                    let speechMs = 0;
                    let vadEnabled = false;

                    const SAMPLE_RATE = 16000;
                    const FRAME_MS = 20;
                    const MIN_SPEECH_MS = 200;     // Require more speech before triggering
                    const END_SILENCE_MS = 1200;   // Wait longer before ending utterance
                    const ENERGY_ON = 0.008;       // Slightly higher threshold to start
                    const ENERGY_OFF = 0.004;      // Slightly higher threshold to stop
                    const PRE_ROLL_MS = 300;

                    // Ring buffer for pre-roll
                    const preRollFrames = [];
                    const maxPreRollFrames = Math.ceil(PRE_ROLL_MS / FRAME_MS);

                    // VAD Worklet processor code
                    const workletCode = `
                        class VADProcessor extends AudioWorkletProcessor {
                            constructor(options) {
                                super();
                                this.frameSize = options.processorOptions.frameSize || 320;
                                this.buffer = new Float32Array(0);
                            }

                            process(inputs) {
                                const input = inputs[0];
                                if (!input || input.length === 0) return true;

                                const ch0 = input[0];
                                const newBuf = new Float32Array(this.buffer.length + ch0.length);
                                newBuf.set(this.buffer, 0);
                                newBuf.set(ch0, this.buffer.length);
                                this.buffer = newBuf;

                                while (this.buffer.length >= this.frameSize) {
                                    const frame = this.buffer.subarray(0, this.frameSize);
                                    this.buffer = this.buffer.subarray(this.frameSize);

                                    // Simple energy (RMS) calculation
                                    let sum = 0;
                                    for (let i = 0; i < frame.length; i++) sum += frame[i] * frame[i];
                                    const rms = Math.sqrt(sum / frame.length);

                                    // Convert float32 [-1..1] to int16
                                    const pcm = new Int16Array(frame.length);
                                    for (let i = 0; i < frame.length; i++) {
                                        const s = Math.max(-1, Math.min(1, frame[i]));
                                        pcm[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
                                    }

                                    this.port.postMessage({ pcm, energy: rms });
                                }
                                return true;
                            }
                        }
                        registerProcessor('vad-processor', VADProcessor);
                    `;

                    async function startVAD() {
                        try {
                            console.log('üé§ Starting VAD...');

                            // Create audio context
                            audioCtx = new AudioContext({ sampleRate: SAMPLE_RATE });

                            // Add worklet module
                            const blob = new Blob([workletCode], { type: 'application/javascript' });
                            const workletUrl = URL.createObjectURL(blob);
                            await audioCtx.audioWorklet.addModule(workletUrl);

                            // Get microphone
                            mic = await navigator.mediaDevices.getUserMedia({
                                audio: {
                                    echoCancellation: true,
                                    noiseSuppression: true,
                                    autoGainControl: true
                                },
                                video: false
                            });

                            // Create audio nodes
                            const source = audioCtx.createMediaStreamSource(mic);
                            workletNode = new AudioWorkletNode(audioCtx, 'vad-processor', {
                                processorOptions: { frameSize: Math.floor(SAMPLE_RATE * FRAME_MS / 1000) }
                            });

                            source.connect(workletNode);

                            // Connect to WebSocket
                            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                            const wsUrl = `${protocol}//${window.location.host}/ws/vad-stream`;
                            console.log('üîó Connecting to WebSocket:', wsUrl);
                            socket = new WebSocket(wsUrl);

                            socket.onopen = () => {
                                console.log('‚úÖ VAD WebSocket connected');
                                updateVADStatus('üü¢ Voice Mode Active');
                            };

                            socket.onmessage = (event) => {
                                try {
                                    const data = JSON.parse(event.data);
                                    console.log('üì® Received WebSocket message:', data);

                                    if (data.type === 'TRANSCRIPT') {
                                        console.log('üìù Received transcript:', data.text);
                                        addTranscriptToInput(data.text);
                                    } else if (data.type === 'STATUS') {
                                        console.log('‚ÑπÔ∏è Status:', data.message);
                                    } else if (data.type === 'ERROR') {
                                        console.error('‚ùå Server error:', data.message);
                                        updateVADStatus('üî¥ Error: ' + data.message);
                                    }
                                } catch (error) {
                                    console.error('‚ùå Failed to parse WebSocket message:', error);
                                }
                            };

                            socket.onerror = (error) => {
                                console.error('‚ùå WebSocket error:', error);
                                updateVADStatus('üî¥ WebSocket Error');
                            };

                            socket.onclose = () => {
                                console.log('üîå WebSocket closed');
                                updateVADStatus('üî¥ Voice Mode Off');
                                vadEnabled = false;
                            };

                            // Handle audio processing
                            workletNode.port.onmessage = (event) => {
                                const { pcm, energy } = event.data;

                                // Keep pre-roll
                                preRollFrames.push(pcm);
                                if (preRollFrames.length > maxPreRollFrames) preRollFrames.shift();

                                // VAD logic with hysteresis
                                if (!speaking) {
                                    if (energy > ENERGY_ON) {
                                        speechMs += FRAME_MS;
                                        if (speechMs >= MIN_SPEECH_MS) {
                                            speaking = true;
                                            silenceMs = 0;
                                            console.log('üéôÔ∏è Speech detected');
                                            updateVADStatus('üü† Listening...');

                                            // Send pre-roll frames
                                            for (const frame of preRollFrames) {
                                                socket.send(frame.buffer);
                                            }
                                            preRollFrames.length = 0;
                                        }
                                    } else {
                                        speechMs = Math.max(0, speechMs - FRAME_MS);
                                    }
                                }

                                if (speaking) {
                                    socket.send(pcm.buffer);
                                    if (energy < ENERGY_OFF) {
                                        silenceMs += FRAME_MS;
                                        if (silenceMs >= END_SILENCE_MS) {
                                            speaking = false;
                                            speechMs = 0;
                                            silenceMs = 0;
                                            console.log('üîá Speech ended');
                                            updateVADStatus('üü¢ Voice Mode Active');

                                            // Signal utterance end
                                            socket.send(JSON.stringify({ type: 'UTTERANCE_END' }));
                                        }
                                    } else {
                                        silenceMs = 0;
                                    }
                                }
                            };

                            vadEnabled = true;
                            return true;

                        } catch (error) {
                            console.error('‚ùå VAD start error:', error);
                            updateVADStatus('üî¥ VAD Start Failed: ' + error.message);
                            return false;
                        }
                    }

                    function stopVAD() {
                        try {
                            console.log('üõë Stopping VAD...');

                            if (workletNode) workletNode.port.postMessage({ type: 'STOP' });
                            if (mic) mic.getTracks().forEach(t => t.stop());
                            if (audioCtx) audioCtx.close();
                            if (socket) socket.close();

                            speaking = false;
                            silenceMs = 0;
                            speechMs = 0;
                            preRollFrames.length = 0;
                            vadEnabled = false;

                            updateVADStatus('üî¥ Voice Mode Off');

                        } catch (error) {
                            console.error('‚ùå VAD stop error:', error);
                        }
                    }

                    function updateVADStatus(status) {
                        // Find the VAD status textbox and update it
                        console.log('üîÑ Updating VAD status to:', status);

                        // More robust selector approach
                        const statusInputs = document.querySelectorAll('input[type="text"], textarea');
                        statusInputs.forEach(input => {
                            const label = input.parentElement.querySelector('label');
                            if (label && label.textContent.includes('Voice Mode')) {
                                input.value = status;
                                input.dispatchEvent(new Event('input', { bubbles: true }));
                                input.dispatchEvent(new Event('change', { bubbles: true }));
                                console.log('‚úÖ Updated VAD status input');
                            }
                        });

                        // Also try to find by data attributes or nearby text
                        const allInputs = document.querySelectorAll('input[type="text"]');
                        allInputs.forEach(input => {
                            if (input.value && (input.value.includes('Voice Mode') || input.value.includes('üî¥') || input.value.includes('üü¢'))) {
                                input.value = status;
                                input.dispatchEvent(new Event('input', { bubbles: true }));
                                console.log('‚úÖ Updated status by content match');
                            }
                        });
                    }

                    function addTranscriptToInput(text) {
                        console.log('üìù Adding transcript to input:', text);

                        // Filter out very short or common spurious transcriptions
                        const spuriousWords = ['thank you', 'thanks', 'yeah', 'yes', 'ok', 'okay', 'um', 'uh'];
                        const isSpurious = text.trim().length < 4 ||
                                         spuriousWords.includes(text.trim().toLowerCase()) ||
                                         text.trim().split(' ').length < 2; // Require at least 2 words

                        if (isSpurious) {
                            console.log('‚ö†Ô∏è Filtering out spurious transcription:', text);
                            return;
                        }

                        // More robust approach to find the message input
                        let messageInput = null;

                        // Method 1: Look for textareas (Gradio often uses these)
                        const textareas = document.querySelectorAll('textarea');
                        textareas.forEach(textarea => {
                            const placeholder = textarea.placeholder || '';
                            const label = textarea.closest('.gradio-container')?.querySelector('label')?.textContent || '';

                            if (placeholder.includes('Type your message') ||
                                placeholder.includes('message') ||
                                label.includes('Message')) {

                                messageInput = textarea;
                            }
                        });

                        // Method 2: Look for any input that might be the message field
                        if (!messageInput) {
                            const inputs = document.querySelectorAll('input[type="text"]');
                            inputs.forEach(input => {
                                const placeholder = input.placeholder || '';
                                const label = input.closest('.gradio-container')?.querySelector('label')?.textContent || '';

                                if (placeholder.includes('Type your message') ||
                                    placeholder.includes('message') ||
                                    label.includes('Message')) {

                                    messageInput = input;
                                }
                            });
                        }

                        // Method 3: Fallback - look for the first textarea/input that looks like a message field
                        if (!messageInput) {
                            const allInputs = document.querySelectorAll('textarea, input[type="text"]');
                            for (const input of allInputs) {
                                // Skip if it's clearly not the message input (status fields, etc.)
                                const isStatusField = input.value && (
                                    input.value.includes('üü¢') ||
                                    input.value.includes('üî¥') ||
                                    input.value.includes('Agent') ||
                                    input.value.includes('Voice Mode')
                                );

                                if (!isStatusField && !input.disabled && !input.readOnly) {
                                    messageInput = input;
                                    break;
                                }
                            }
                        }

                        if (messageInput) {
                            // Clear the input and set the new text
                            messageInput.value = text.trim();

                            // Trigger multiple events to ensure Gradio detects the change
                            messageInput.dispatchEvent(new Event('input', { bubbles: true }));
                            messageInput.dispatchEvent(new Event('change', { bubbles: true }));
                            messageInput.focus();

                            console.log('‚úÖ Updated input with transcript');

                            // Auto-send the message after a short delay
                            setTimeout(() => {
                                // Find and click the Send button
                                const sendButtons = document.querySelectorAll('button');
                                sendButtons.forEach(button => {
                                    if (button.textContent.includes('Send') ||
                                        button.textContent.includes('send') ||
                                        button.querySelector('[data-testid="send-button"]')) {

                                        console.log('üöÄ Auto-sending voice message');
                                        button.click();
                                    }
                                });
                            }, 500); // Small delay to ensure the input is properly set

                        } else {
                            console.error('‚ùå Could not find message input field');
                            console.log('Available textareas:', document.querySelectorAll('textarea'));
                            console.log('Available text inputs:', document.querySelectorAll('input[type="text"]'));
                        }
                    }

                    // Attach to VAD toggle button
                    document.addEventListener('click', async (event) => {
                        if (event.target.textContent.includes('Voice Mode')) {
                            event.preventDefault();

                            if (!vadEnabled) {
                                updateVADStatus('üü° Starting Voice Mode...');
                                const success = await startVAD();
                                if (!success) {
                                    updateVADStatus('üî¥ Voice Mode Failed');
                                }
                            } else {
                                stopVAD();
                            }
                        }
                    });

                    console.log('‚úÖ VAD JavaScript loaded');
                }
                """,
            )

        return interface


def create_chat_app() -> gr.Blocks:
    """Factory function to create the chat interface"""
    chat_interface = LangGraphChatInterface()
    return chat_interface.create_interface()


# For standalone testing
if __name__ == "__main__":
    app = create_chat_app()
    app.launch(debug=True, share=False)
